<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="webthemez">
    <title>SIGKDD 2025 Tutorial - Model Extraction Attack and Defenses for Large Language Models</title>
    <!-- core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/font-awesome.min.css" rel="stylesheet">
    <link href="css/animate.min.css" rel="stylesheet"> 
    <link href="css/prettyPhoto.css" rel="stylesheet">
    <link href="css/styles.css" rel="stylesheet"> 
    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <script src="js/respond.min.js"></script>
    <![endif]-->       
    <link rel="shortcut icon" href="images/ico/favicon.ico"> 
</head> 

<body id="home">

    <header id="header">
        <!-- 原有导航栏 -->
        <nav id="main-nav" class="navbar navbar-default navbar-fixed-top" role="banner">
            <div class="container">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                
                <div class="collapse navbar-collapse navbar-right">
                    <ul class="nav navbar-nav">
                        <li class="scroll active"><a href="#home">Home</a></li> 
                        <li class="scroll"><a href="#features">Content</a></li>
                        <li class="scroll"><a href="#services">Related Materials</a></li>
                        <li class="scroll"><a href="#about">Key References</a></li> 
                        <li class="scroll"><a href="#our-team">Presenters</a></li>
                        <li class="scroll"><a href="#contact-us">Contact</a></li>                        
                    </ul>
                </div>
            </div><!--/.container-->
        </nav><!--/nav-->
    </header><!--/header-->
                <!-- ✅ 新增横向学校 logo 区块 -->
    <div class="container-fluid" style="padding: 0; background-color: #111;">
        <img src="images/logo2.png" alt="school logos" style="width: 75%; height: auto; display: block;">
    </div>
    
    <section id="hero-banner">
        <div class="banner-inner">
            <div class="container">
                <div class="row">
                    <div class="col-sm-12">
                        <h2>Model Extraction Attack and Defenses for Large Language Models: Recent Advances, Challenges, and Future Prospectives</h2>
                        <p><br/> <b>Time</b>: 02:00 PM - 5:00 PM (PDT), Sunday, August 3. <br/> <b>Location</b>: Room xxx, Metro Toronto Convention Centre, Toronto, Canada.</p>
                        <a class="btn btn-primary btn-lg" href="https://labrai.github.io/files/SIGKDD_2025_tutorial.pdf">Slides</a>
                    </div>
                </div>
            </div>
        </div>
    </section><!--/#main-slider-->

    <section id="features">
        <div class="container">
            <div class="section-header">
                <h2 class="section-title wow fadeInDown">Abstract</h2>
                <p class="wow fadeInDown">Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt security attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt security strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.</p>
            </div>
        </div>
    </section>

    <!-- 以下部分暂时注释掉 -->
    <!--
    <section id="services">...</section>
    <section id="about">...</section>
    <section id="our-team">...</section>
    <section id="contact-us">...</section>
    -->

    <script src="js/jquery.js"></script>
    <script src="js/bootstrap.min.js"></script> 
    <script src="js/mousescroll.js"></script>
    <script src="js/smoothscroll.js"></script>
    <script src="js/jquery.prettyPhoto.js"></script>
    <script src="js/jquery.isotope.min.js"></script>
    <script src="js/jquery.inview.min.js"></script>
    <script src="js/wow.min.js"></script>
    <script src="js/custom-scripts.js"></script>
</body>
</html>
